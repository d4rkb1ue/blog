---
title: [Book learning note] Build a LLM from Scratch
date: 2025-02-20 10:51:48
tags: []
---


1. BERT is a comparable concept of GPT, which mask the word and let the model to predict. Versas GPT to predict the next word.
2. > we can use the next word in a sentence or document as the label that the model is supposed to predict.
    - By saying GPT(and also BERT) is **self-supervised**, it doesn't means that they are learning without any feedback. It still has the feedback by predicting the next word. 

4. 